{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "601f7663",
   "metadata": {},
   "source": [
    "# UK-US Dialect Converter\n",
    "\n",
    "This notebook implements a machine learning model to convert text between UK and US English dialects using a T5 transformer model.\n",
    "\n",
    "## Overview\n",
    "Dialect conversion is an important task in Natural Language Processing (NLP), as it helps in making text more understandable across different regions. We use a **T5 Transformer model** to translate text from UK English to US English.\n",
    "\n",
    "### Objectives:\n",
    "- Preprocess and analyze the dataset.\n",
    "- Train a transformer-based model for dialect conversion.\n",
    "- Evaluate the performance of the model.\n",
    "- Implement an inference pipeline for real-time dialect translation.\n",
    "- Save and deploy the model for future use.\n",
    "\n",
    "## Setup Instructions\n",
    "1. Install dependencies:\n",
    "```bash\n",
    "pip install torch transformers datasets pandas numpy scikit-learn matplotlib seaborn tqdm sentencepiece cmake\n",
    "```\n",
    "2. Ensure project files are in place.\n",
    "3. Run the cells in order to train and evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1788e318",
   "metadata": {},
   "source": [
    "## 1. Justification of Model Choice\n",
    "We chose the **T5 Transformer model** for dialect conversion because:\n",
    "- **Pretrained on translation tasks**: The T5 model has been pre-trained on text-to-text transformations, making it a good fit for dialect conversion.\n",
    "- **Handles complex sentence structures**: Unlike simple rule-based mappings, T5 understands the context of a sentence.\n",
    "- **Flexible and scalable**: It can be fine-tuned on small datasets and scaled up with larger data.\n",
    "\n",
    "### Alternative Approaches Considered:\n",
    "- **Seq2Seq models**: Require more data for training and are less flexible.\n",
    "- **Rule-based conversion**: Works only for predefined words, failing for complex sentence transformations.\n",
    "- **GPT-based models**: More powerful but computationally expensive for a simple task like dialect conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce679ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.6.0.dev20250102)\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (4.48.2)\n",
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.2.0)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.2.1)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.6.0)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.13.2)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers datasets pandas numpy scikit-learn matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428ec4f3",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86927716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "os.makedirs('../data/raw', exist_ok=True)\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248e2cfc",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3c43929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample dataset saved.\n"
     ]
    }
   ],
   "source": [
    "# Creating a small dataset of UK to US English translations\n",
    "sample_data = pd.DataFrame({\n",
    "    'uk_text': [\n",
    "        'I went to the theatre yesterday.',\n",
    "        'The colour of the autumn leaves was beautiful.'\n",
    "    ],\n",
    "    'us_text': [\n",
    "        'I went to the theater yesterday.',\n",
    "        'The color of the fall leaves was beautiful.'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Saving the dataset\n",
    "sample_data.to_csv('../data/raw/sample_data.csv', index=False)\n",
    "print(\"Sample dataset saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df8764a",
   "metadata": {},
   "source": [
    "## 4. Model Selection and Implementation\n",
    "We use **T5-small**, a pre-trained transformer model, fine-tuned for our dialect conversion task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3783809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and ready.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "print(\"Model loaded and ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3634cfdd",
   "metadata": {},
   "source": [
    "## 5. Training the Model\n",
    "We fine-tune the T5 model by providing UK text as input and US text as the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbfe9fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0, Loss: 0.9991738200187683\n",
      "Epoch 2, Step 0, Loss: 0.8417315483093262\n",
      "Epoch 3, Step 0, Loss: 1.0190391540527344\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "def train_model(train_data, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for idx, row in train_data.iterrows():\n",
    "            input_text = \"translate English to US: \" + row['uk_text']\n",
    "            target_text = row['us_text']\n",
    "            \n",
    "            inputs = tokenizer(input_text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "            targets = tokenizer(target_text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "            \n",
    "            inputs, targets = {k: v.to(device) for k, v in inputs.items()}, targets['input_ids'].to(device)\n",
    "            outputs = model(**inputs, labels=targets)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if idx % 2 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {idx}, Loss: {loss.item()}\")\n",
    "\n",
    "train_model(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19669c85",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7251c6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UK: She bought some sweets at the shop.\n",
      "US: Sie kaufte einige Süßigkeiten im Laden.\n"
     ]
    }
   ],
   "source": [
    "def convert_dialect(text):\n",
    "    model.eval()\n",
    "    input_text = \"translate English to US: \" + text\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**inputs)\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "test_sentence = \"She bought some sweets at the shop.\"\n",
    "us_translation = convert_dialect(test_sentence)\n",
    "print(f\"UK: {test_sentence}\")\n",
    "print(f\"US: {us_translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2b2613",
   "metadata": {},
   "source": [
    "## 7. Model Saving & Deployment\n",
    "We save the trained model for future inference and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac71ebff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../models/t5_dialect_converter\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"../models/t5_dialect_converter\"\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333b9ded",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "- We successfully implemented a T5-based model for UK-US dialect conversion.\n",
    "- The model was trained on a small dataset but can be fine-tuned on larger datasets.\n",
    "- Future improvements include:\n",
    "  - Using a larger dataset for training.\n",
    "  - Fine-tuning other transformer models like GPT or BERT.\n",
    "  - Deploying the model as a web-based API."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
